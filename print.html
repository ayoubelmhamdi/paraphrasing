<!DOCTYPE HTML>
<html lang="En" class="sidebar-visible no-js navy">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Paraphrasing the Pdfs</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
        
        <link rel="stylesheet" href="css/tavianator.css">
        
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
        
        <script src="tavianator.js"></script>
        
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-EBDQXGZ6R1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-EBDQXGZ6R1');
        </script>

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="./theme/tabbed-code-blocks.css">

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "navy";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('navy')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> INTRODUCTION</a></li><li class="chapter-item expanded "><a href="lung1.html"><strong aria-hidden="true">2.</strong> Learning Efficient, Explainable and Discriminative Representations for Pulmonary Nodules Classification</a></li><li class="chapter-item expanded "><a href="seance-1-les-ondes-mecaniques-progressives-8.html"><strong aria-hidden="true">3.</strong> seance-1-les-ondes-mecaniques-progressives-8</a></li><li class="chapter-item expanded "><a href="2207.14238.html"><strong aria-hidden="true">4.</strong> 2207.14238</a></li><li class="chapter-item expanded "><a href="2308.14963.html"><strong aria-hidden="true">5.</strong> 2308.14963</a></li><li class="chapter-item expanded "><a href="s3-tabi-c.html"><strong aria-hidden="true">6.</strong> s3-tabi-c</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Paraphrasing the Pdfs</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/ayoubelmhamdi/paraphrasing" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="paraphrasing-some-usefull-pdfs"><a class="header" href="#paraphrasing-some-usefull-pdfs">PARAPHRASING SOME USEFULL PDFs</a></h1>
<hr />
<p>This simplify use Chatgpt3.5</p>
<ul>
<li><a href="./lung1.html">Learning Efficient, Explainable and Discriminative Representations for Pulmonary Nodules Classification</a></li>
<li><a href="./seance-1-les-ondes-mecaniques-progressives-8.html">seance-1-les-ondes-mecaniques-progressives-8</a></li>
<li><a href="./2207.14238.html">2207.14238</a></li>
<li><a href="./2308.14963.html">2308.14963</a></li>
<li><a href="./s3-tabi-c.html">s3-tabi-c</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<p>Title: Automated Lung Nodule Detection and Classification Using Deep Learning</p>
<p>Abstract: Lung cancer is a deadly disease that is often detected at advanced stages. Early detection is crucial for survival, but it can be challenging because benign and malignant nodules look similar. This paper proposes a new deep learning model that uses multiple strategies to accurately diagnose malignant nodules. The model uses advanced image analysis techniques and machine learning algorithms to detect and classify nodules. To reduce errors, the model also considers physiological symptoms and clinical biomarkers. The proposed system was tested on real datasets and achieved better results compared to existing methods.</p>
<p>Introduction: Lung cancer is a serious disease that can be deadly if not detected early. Pulmonary nodules are small growths in the lungs that can be cancerous or noncancerous. Detecting cancerous nodules early is important for prognosis. However, it can be difficult to differentiate between cancerous and noncancerous nodules based on their appearance and clinical biomarkers. Physicians use various diagnostic procedures, but invasive methods like biopsies are often needed. Computed tomography (CT) imaging is the most effective method for investigating lung diseases, but it has limitations such as false positive findings and radiation exposure. Low-dose CT scans have been shown to reduce cancer-related deaths. However, analyzing CT scans is time-consuming and prone to errors. Computer-aided detection (CAD) systems have been developed to assist radiologists, but they still struggle to accurately classify nodules. Benign and malignant nodules have similar features, but there are subtle differences in their appearance and location.</p>
<p>Conclusion: Early detection of lung cancer is crucial for improving survival rates. This paper proposes a deep learning model that uses advanced image analysis techniques and machine learning algorithms to detect and classify malignant nodules. The model takes into account physiological symptoms and clinical biomarkers to reduce errors. The proposed system achieved better results compared to existing methods.</p>
<p>This paper is about using deep learning and multiple strategies to detect and classify lung nodules. During the screening process, errors can occur that lead to missed or incorrect diagnoses. These errors can be reduced by using deep learning models that analyze CT scans along with clinical and physiological findings. The paper discusses different deep learning architectures, such as ResNet and DenseNet, that have been used for this purpose. The use of Internet of Things (IoT) and medical sensor devices has also improved the detection and classification of lung cancer. The proposed system in this paper uses deep learning models on 3D lung CT scans, along with physiological symptoms and clinical biomarkers, to reduce false positive results. The paper describes the design of the deep learning models for nodule detection and classification, as well as the experimental results. The conclusion of the paper is presented in the last section.</p>
<p>The researchers in this study developed a computer program using deep learning to detect and classify lung nodules in CT scans. They used different strategies to improve the accuracy of the program.</p>
<p>One strategy involved using a 3D deep convolutional neural network (CNN) to analyze the CT images and predict the presence of nodules. Another strategy involved combining features from CT and PET images to identify suspicious areas in the lungs.</p>
<p>The researchers also used proteomic classifiers, which are proteins found in the blood, to differentiate between benign and malignant nodules. They found that certain proteins, along with other risk factors like age and smoking history, were effective in classifying the nodules.</p>
<p>The researchers compared their program to the performance of radiologists and found that it had better results in detecting nodules. However, the program still had difficulty detecting nodules smaller than 6 mm.</p>
<p>To improve the accuracy of the program, the researchers proposed using a combination of strategies, including the deep learning program and biomarkers from blood tests. They found that this multi-strategy approach could reduce false positive results and improve the detection and diagnosis of lung nodules.</p>
<p>The proposed system works by analyzing physiological symptoms, CT scans, and clinical biomarkers to make decisions about the presence and classification of lung nodules. The system uses deep learning algorithms to analyze the CT scans and clinical biomarkers to classify the nodules.</p>
<p>The researchers also provided an overview structure of the proposed system, which includes a CT scan machine, a deep neural network for data analysis, and body sensor devices for data collection. The system collects patient data and makes diagnostic decisions based on the analysis of the CT scans and clinical biomarkers.</p>
<p>Overall, the researchers aim to improve the accuracy of lung nodule detection and classification by combining different strategies and using advanced technology like deep learning and biomarkers.</p>
<p>This text is about a system that uses deep learning and multiple strategies to detect and classify lung nodules. It explains how physiological symptoms can be monitored remotely using wearable devices and IoT technology. The text also mentions the use of sensor devices and smartphone applications to gather physiological information and send it to a remote cloud for analysis. The research was conducted at a healthcare center in China, and various symptoms of lung cancer at different stages are listed in a table. The text then discusses the use of deep learning for CT scan analysis, specifically for nodule detection and classification. It mentions the use of a 3D Faster R-CNN-like architecture and a network called CMixNet for learning nodule features.</p>
<p>ResNet, DenseNet, DPN, MixNet are different types of modern CNN connection architectures.</p>
<p>In DenseNet, the features from previous layers are combined with the new layer using concatenation. This helps improve the flow of information and increase the feature dimensions. However, it can also lead to redundancy.</p>
<p>MixNet introduces &quot;shifted additions&quot; to address the redundancy issue. It uses a mixed link connection structure that combines inner and outer link modules.</p>
<p>In ResNet, the previous feature map is added to the current output. MixNet uses flexible inner and outer link modules to make the architecture stronger.</p>
<p>The nodule detection task is performed using a combination of CMixNet and U-Net. CMixNet is used for feature extraction, while U-Net is used for region proposal.</p>
<p>The detection process involves extracting features, proposing regions, and performing classification. Different sizes of anchor boxes are used based on the distribution of nodule sizes.</p>
<p>The loss function for each anchor box includes classification loss and regression loss. The classification loss determines if a box contains a nodule, while the regression loss determines the size and position of the nodule.</p>
<p>CMixNet consists of multiple blocks, each with several layers. The output feature maps at different stages are shown in Table 3.</p>
<p>The feature extraction process and nodule detection using faster R-CNN are depicted in Figure 6 and Figure 7, respectively.</p>
<p>The multi-task loss function includes L1 and Focal Losses, which are used to evaluate regression and classification errors.</p>
<p>ResNet, DenseNet, DPN, MixNet are different types of deep learning models used for image classification.</p>
<p>The figure shows how object detection works. The region proposal network generates a map of features, which is then processed by the MixNet model.</p>
<p>Figure 7 demonstrates how nodule detection is done using Faster R-CNN.</p>
<p>In order to differentiate between early-stage malignant and benign lung nodules, lung CT images are cropped and processed using convolutional neural networks. A modified version of the MixNet model called CMixNet is used to extract features from the lung nodules. These features are then used to classify the nodules as malignant or benign using logistic regression and GBM.</p>
<p>The sizes of the feature maps and filters used in the CMixNet model are shown in Figure 8.</p>
<p>Clinical biomarkers, such as blood tests and proteins, can help in the early diagnosis of lung cancer. These biomarkers are present in body fluids like blood or saliva. Protein biomarkers are particularly useful for lung cancer diagnosis because they are involved in cellular processes. Table 4 shows some important clinical biomarkers and their sensitivity and specificity for detecting cancer.</p>
<p>Experiments were conducted to train and evaluate the performance of the proposed models. The LUNA16 and LIDC-IDRI datasets were used for training and testing. The models were implemented on a NVIDIA TESLA K80 GPU.</p>
<p>Preprocessing techniques were used to prepare the CT scans for nodule detection and classification. The lung parenchyma was segmented from the scans based on radiodensities.</p>
<p>The nodule detection model was trained using the LUNA16 dataset, while the nodule classification model was trained using the LIDC-IDRI dataset. Different augmentation techniques were used to address class imbalance and increase the training data.</p>
<p>The models were trained using SGD with mini-batch size and multiple learning rates. ResNeXt with full pre-activation was used to improve training performance.</p>
<p>The LIDC-IDRI dataset was divided into ten sets for cross-validation. The weights of the models were learned using SGD.</p>
<p>ResNet, DenseNet, DPN, MixNet are different types of deep learning models.</p>
<p>The goal of training is to make the network's output as close as possible to the correct answer. There are two main reasons to stop training: when the network has reached its lowest error rate, or when the loss function on the validation data stops changing.</p>
<p>In the nodule detection part of the system, the detector was trained on a dataset called LUNA16. The performance of the system was evaluated using a metric called FROC, which measures the sensitivity (ability to detect true positives) versus the false positive rate.</p>
<p>The system achieved a high FROC score of 94.21% using a model called 3D CMixNet with Faster R-CNN. This was better than other models like 3D MixNet and 3D DPN26.</p>
<p>In the nodule classification part, the system was evaluated on a dataset called LIDC-IDRI. The system achieved higher accuracy with fewer parameters compared to other models.</p>
<p>Automated lung nodule detection and classification techniques are good at finding nodules but often have false positive results. Clinical biomarkers have low detection capability but high specificity. Combining the two can help reduce false positives and make better decisions.</p>
<p>The performance of the system was evaluated using statistical measures like sensitivity, specificity, accuracy, and AUC (area under the curve). The proposed model achieved good results compared to other models.</p>
<p>The computational complexity of the system was evaluated in terms of the number of parameters and training time. The proposed model had a low computational cost and trained in about a week.</p>
<p>Overall, the proposed system achieved good results in nodule detection and classification with low computational cost.</p>
<p>This study proposes a system for detecting and classifying lung nodules using multiple strategies. The system aims to reduce false positives in the early stages of detection. It analyzes 3D lung CT images to identify malignant nodules. First, it uses a technique called 3D Faster R-CNN with CMixNet and a U-Net-like encoder-decoder to detect nodules. Then, it uses 3D CMixNet with GBM to classify the nodules as benign or malignant. The results of the deep learning-based classification are evaluated using factors such as patient history, age, smoking history, clinical biomarkers, size, and location of the nodules. The system was tested on publicly available datasets and showed better performance with less computational cost. The study was supported by funding from the National Natural Science Foundation of China and the Chongqing Research Program of Basic Science and Frontier Technology. The authors would like to thank the Vice President and Director of Chongqing (CHN.USA) Hygeia Cancer Hospital for providing clinical data. The authors declare no conflicts of interest.</p>
<p>ResNet, DenseNet, DPN, and MixNet are names of different types of computer algorithms used for analyzing data.</p>
<p>In a study published in the Lung Cancer journal in 2013, researchers investigated the usefulness of two tumor markers, CEA and CYFRA 21-1, in diagnosing lung cancer.</p>
<p>Another study published in the Dis. Markers journal in 2018 assessed the effectiveness of seven clinical tumor markers in diagnosing non-small-cell lung cancer.</p>
<p>Neuron-specific enolase and chromogranin A are markers that can indicate the presence of neuroendocrine tumors, according to a study published in the British Journal of Cancer in 1998.</p>
<p>In a research paper published on arXiv in 2017, the authors used deep learning techniques to detect lung cancer as part of the Kaggle Data Science Bowl 2017 challenge.</p>
<p>The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) have created a reference database of lung nodules on CT scans, as described in a paper published in Medical Physics in 2011.</p>
<p>A study published in the Information Processing in Medical Imaging journal in 2015 proposed the use of multi-scale convolutional neural networks for classifying lung nodules.</p>
<p>The book &quot;Computer Vision—ACCV 2016&quot; contains research on computer vision, including topics related to lung cancer detection.</p>
<p>Another study published in the Pattern Recognition journal in 2017 suggested the use of multi-crop convolutional neural networks for classifying the malignancy of lung nodules.</p>
<p>In a conference paper presented at the 2017 IEEE 14th International Symposium on Biomedical Imaging, the authors introduced a lung nodule characterization method called TuamorNet, which uses a multi-view convolutional neural network with Gaussian process.</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="physique-et-chimie--2ème-année-bac---les-ondes-mécaniques-progressives"><a class="header" href="#physique-et-chimie--2ème-année-bac---les-ondes-mécaniques-progressives">Physique et Chimie : 2ème Année Bac - Les ondes mécaniques progressives</a></h1>
<h2 id="i--définitions"><a class="header" href="#i--définitions">I- Définitions</a></h2>
<p>1-1/ Onde mécanique
Une onde mécanique est un phénomène où une perturbation se propage dans un milieu matériel sans transporter de matière, mais en transportant de l'énergie.
1-2/ Milieu matériel élastique
Un milieu matériel élastique est capable de retrouver sa forme initiale après avoir subi le passage d'une onde.</p>
<h2 id="ii--types-dondes"><a class="header" href="#ii--types-dondes">II- Types d'ondes</a></h2>
<h3 id="2-1-onde-transversale"><a class="header" href="#2-1-onde-transversale">2-1/ Onde transversale</a></h3>
<p>Une onde est transversale lorsque la perturbation du milieu est perpendiculaire à la direction de propagation de l'onde. Par exemple, une onde le long d'une corde ou à la surface de l'eau.</p>
<h3 id="2-2-onde-longitudinale"><a class="header" href="#2-2-onde-longitudinale">2-2/ Onde longitudinale</a></h3>
<p>Une onde est longitudinale lorsque la perturbation du milieu est alignée avec la direction de propagation de l'onde. Par exemple, une onde le long d'un ressort ou une onde sonore.</p>
<h2 id="iii--londe-mécanique-progressive"><a class="header" href="#iii--londe-mécanique-progressive">III- L'onde mécanique progressive</a></h2>
<p>Une onde mécanique progressive est une succession de signaux mécaniques qui se propagent dans un milieu supposé infini. Elle peut être générée par la vibration d'une source, comme des gouttes d'eau tombant à la surface d'un étang.</p>
<h2 id="iv--londe-sonore"><a class="header" href="#iv--londe-sonore">IV- L'onde sonore</a></h2>
<p>Une onde sonore est une onde mécanique progressive longitudinale qui se propage dans les milieux matériels (solide, liquide et gaz) mais pas dans le vide. Elle se propage grâce à la compression et la dilatation du milieu de propagation.</p>
<h2 id="v--propriétés-des-ondes-mécaniques"><a class="header" href="#v--propriétés-des-ondes-mécaniques">V- Propriétés des ondes mécaniques</a></h2>
<p>5-1/ Dimension d'onde
Une onde mécanique peut avoir une, deux ou trois dimensions selon la façon dont elle se propage. Par exemple, une onde le long d'une corde a une dimension, une onde à la surface de l'eau a deux dimensions, et une onde sonore a trois dimensions.</p>
<p>5-2/ Superposition d'ondes
Lorsque plusieurs ondes se propagent dans la même région, elles conservent leur intégrité et s'éloignent sans être altérées. Leurs amplitudes s'ajoutent algébriquement.</p>
<h2 id="vi--vitesse-de-propagation-dune-onde"><a class="header" href="#vi--vitesse-de-propagation-dune-onde">VI- Vitesse de propagation d'une onde</a></h2>
<p>6-1/ Définition
La vitesse de propagation d'une onde (appelée célérité) est égale à la distance parcourue divisée par le temps mis pour la parcourir. Elle peut être calculée en utilisant la formule V = d/t.</p>
<p>6-2/ Facteurs influençant la célérité de propagation
La célérité d'une onde dépend de la nature du milieu de propagation. Elle augmente avec l'élasticité du milieu et diminue avec son inertie. Pour une onde sonore, la célérité augmente avec la densité du milieu et la température.</p>
<h2 id="vii--notion-de-retard"><a class="header" href="#vii--notion-de-retard">VII- Notion de retard</a></h2>
<p>Lors de la propagation d'une onde mécanique, tous les points du milieu de propagation subissent la même perturbation que la source, mais avec un retard. Le retard d'un point par rapport à un autre est donné par la distance entre les deux points divisée par la célérité de l'onde.</p>
<h2 id="viii--exercices"><a class="header" href="#viii--exercices">VIII- Exercices</a></h2>
<p>8-1/ Exercice 1
Dans cet exercice, une onde transversale se propage le long d'une corde élastique. On peut déterminer sa célérité en calculant la distance parcourue divisée par le temps mis pour la parcourir. On peut également mesurer la durée de la perturbation d'un point de la corde en observant la figure. Pour dessiner l'aspect de la corde à un instant précis, il faudrait avoir plus d'informations sur la forme de l'onde à cet instant.</p>
<p>Exercice 4
Cet exercice concerne la propagation des ondes sonores dans l'air. On peut déterminer si le son est une onde longitudinale ou transversale en observant la variation de la tension aux bornes des microphones. Le retard temporel entre les microphones peut être calculé en utilisant la distance entre eux et la célérité de propagation du son dans l'air. On peut également comparer la célérité de propagation du son dans l'air avec celle dans l'eau pour en déduire des informations.</p>
<p>Exercice 4 : Vitesse de propagation d'un signal le long d'une corde</p>
<p>La vitesse de propagation d'un signal transversal le long d'une corde tendue peut être calculée à l'aide de la formule v = √(T/µ), où v est la vitesse, T est la tension de la corde et µ est sa masse linéaire.</p>
<ol>
<li>
<p>Pour calculer la vitesse de propagation d'un signal le long d'une corde de longueur L = 10 m et de masse m = 1 kg, avec une tension T = 2,5 N, nous utilisons la formule v = √(T/µ). En substituant les valeurs, nous obtenons v = √(2,5/1) = √2,5 ≈ 1,58 m/s. La durée que met le signal pour parcourir la corde entière est donnée par la formule t = L/v, où t est la durée et L est la longueur de la corde. En substituant les valeurs, nous obtenons t = 10/1,58 ≈ 6,33 s.</p>
</li>
<li>
<p>Si nous utilisons la même corde attachée avec une force 4 fois supérieure à la force précédente, la tension T sera multipliée par 4. En utilisant la formule v = √(T/µ), nous pouvons calculer la nouvelle vitesse de propagation. En substituant les valeurs, nous obtenons v = √((4*2,5)/1) = √10 ≈ 3,16 m/s. Ainsi, la vitesse de propagation du signal sera plus élevée.</p>
</li>
<li>
<p>Si nous tendons la corde avec une masse marquée de 10 kg, nous devons d'abord convertir cette masse en tension en utilisant la formule T = m<em>g, où T est la tension, m est la masse et g est l'accélération due à la gravité. En substituant les valeurs, nous obtenons T = 10</em>10 = 100 N. En utilisant la formule v = √(T/µ), nous pouvons calculer la célérité de la propagation de l'onde le long de la corde. En substituant les valeurs, nous obtenons v = √(100/1) = √100 = 10 m/s. Ainsi, la célérité de l'onde sera de 10 m/s.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="re-thinking-and-re-labeling-lidc-idri-for-robust-pulmonary-cancer-prediction"><a class="header" href="#re-thinking-and-re-labeling-lidc-idri-for-robust-pulmonary-cancer-prediction">Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction</a></h1>
<p><strong>Authors:</strong> Hanxiao Zhang, Xiao Gu, Minghui Zhang, Weihao Yu, Liang Chen, Zhexin Wang, Feng Yao, Yun Gu, and Guang-Zhong Yang</p>
<p><strong>Affiliations:</strong></p>
<ol>
<li>Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China</li>
<li>Imperial College London, London, UK</li>
<li>Department of Thoracic Surgery, Shanghai Chest Hospital, Shanghai Jiao Tong University, Shanghai, China</li>
<li>Shanghai Center for Brain Science and Brain-Inspired Technology, Shanghai, China</li>
</ol>
<p><strong>Abstract:</strong>
The LIDC-IDRI database is commonly used for lung cancer prediction. However, the subjective assessment of nodules by radiologists in this database can lead to label assignment errors and bias during training. To address this, we propose re-labeling the LIDC data using a small dataset of nodules diagnosed by pathological examination. We demonstrate that re-labeling the LIDC nodules using metric learning can improve model performance. We also suggest that building a larger database of nodules with pathological-proven labels is a long-term solution for robust lung cancer prediction.</p>
<p><strong>Keywords:</strong>
Pulmonary nodule, Cancer prediction, Metric learning, Re-labeling.</p>
<h2 id="1-introduction"><a class="header" href="#1-introduction">1 Introduction</a></h2>
<p>The LIDC-IDRI database is widely used for lung nodule detection and cancer prediction. However, there are potential issues with the subjective assessment of nodules by radiologists in this database. The assigned malignancy scores may not accurately reflect the true nature of the nodules, leading to label assignment errors and uncertainty. To address this, we introduce the SCH-LND dataset, which contains nodules with pathological-proven labels. We evaluate the performance of the LIDC-driven model using the SCH-LND dataset and propose re-labeling strategies based on interactions with the SCH-LND dataset. We show that re-labeling the LIDC nodules using metric learning improves model performance, especially when new labels are added to the uncertain subset. We also highlight the need for a larger nodule database with pathological-proven labels.</p>
<h2 id="materials"><a class="header" href="#materials">Materials</a></h2>
<h3 id="lidc-idri-database"><a class="header" href="#lidc-idri-database">LIDC-IDRI Database</a></h3>
<p>We selected nodules identified by at least three radiologists and excluded CT scans with a slice thickness larger than 3 mm. We focused on solid nodules in both the SCH-LND and LIDC databases.</p>
<h3 id="extra-dataset-sch-lnd"><a class="header" href="#extra-dataset-sch-lnd">Extra Dataset: SCH-LND</a></h3>
<p>The SCH-LND dataset consists of 180 solid nodules, with 90 benign and 90 malignant nodules. Each nodule in this dataset has been confirmed and diagnosed through immediate pathological examination via biopsy.</p>
<h2 id="study-design"><a class="header" href="#study-design">Study Design</a></h2>
<p>We conducted training and testing in different scenarios and cases to evaluate the performance of the LIDC-driven model in predicting nodule malignancy. We also explored the use of transfer learning to fine-tune the model based on pre-trained LIDC models. We proposed two re-labeling strategies for the LIDC nodules based on interactions with the SCH-LND dataset. The models trained with re-labeled LIDC data created by the metric learning model outperformed the original model, especially when new labels were added to the uncertain subset. Statistical analysis revealed a class imbalance problem in the re-labeled LIDC data, highlighting the need for a larger nodule database with pathological-proven labels.</p>
<h2 id="performance-comparisons"><a class="header" href="#performance-comparisons">Performance Comparisons</a></h2>
<h1 id="re-thinking-and-re-labeling-lidc-idri-for-robust-pulmonary-cancer-prediction-1"><a class="header" href="#re-thinking-and-re-labeling-lidc-idri-for-robust-pulmonary-cancer-prediction-1">Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction</a></h1>
<h2 id="performance-comparisons-1"><a class="header" href="#performance-comparisons-1">Performance Comparisons</a></h2>
<p>The authors compare the performance of different scenarios and cases in predicting whether nodules are malignant. They use various evaluation metrics such as sensitivity, specificity, precision, accuracy, and F1 score. The results show that there is a bias problem in some scenarios, but others have less bias. They also find that training from scratch on a different dataset does not result in a high-performing model. Therefore, they use transfer learning to fine-tune a pre-trained model.</p>
<h2 id="relabeling-strategies"><a class="header" href="#relabeling-strategies">Relabeling Strategies</a></h2>
<p>The authors propose two strategies to obtain new ground truth labels for the LIDC database. These strategies aim to correct the bias in the assessment of nodules and utilize uncertain nodules with an average score of 3. The details of these strategies are explained in the following section.</p>
<h2 id="methods"><a class="header" href="#methods">Methods</a></h2>
<p>The authors present two re-labeling strategies to obtain new ground truth labels for the LIDC database. The first strategy generates the malignancy label using a machine annotator that has been trained on LIDC data and fine-tuned on a different dataset. The second strategy involves ranking the labels of top nodules using a metric-based network that measures the correlation between pairs of nodules. Two modes of re-labeling are proposed in each strategy.</p>
<p>For Mode 1 (Substitute), the re-label outcomes from other label machines are accepted. For Mode 2 (Consensus), the final re-label results are decided by the consensus of label machine outcomes and the original label. This mode leaves behind nodules with the same label and discards controversial ones, which may reduce the amount of data.</p>
<p>The effectiveness of the re-labeling is evaluated using a different dataset to test the model trained with the re-labeled data.</p>
<h2 id="a-machine-annotator-the-state-of-the-art-nodule-classifier"><a class="header" href="#a-machine-annotator-the-state-of-the-art-nodule-classifier">A Machine Annotator: The State-of-the-Art Nodule Classifier</a></h2>
<p>The machine annotator is a state-of-the-art nodule classifier that has been pre-trained on LIDC data and fine-tuned on a different dataset to predict nodule class. The second strategy involves using a metric learning model to search for the most similar nodules and give new labels. The knowledge from radiologists' assessments is also considered in the re-labeling process.</p>
<h2 id="label-induction-using-machine-annotator"><a class="header" href="#label-induction-using-machine-annotator">Label Induction Using Machine Annotator</a></h2>
<p>The authors use a Siamese Network to train a machine annotator. The network takes pairs of nodules and their labels as inputs and generates feature vectors for each nodule. The goal is to learn representations that can distinguish between nodules of the same class and different classes. During re-labeling, the machine annotator is used to pair each nodule from the training dataset with an under-labeled LIDC nodule and assign a new label based on the similarity scores.</p>
<h2 id="experiments-and-results"><a class="header" href="#experiments-and-results">Experiments and Results</a></h2>
<p>The authors conducted experiments to evaluate the re-labeling strategies. They used different evaluation metrics to measure the performance of each method. The results are presented in tables, showing the sensitivity, specificity, precision, accuracy, and F1 score for each method.</p>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>The authors used a 3D ResNet-18 model for their experiments. They trained the models using the Adam optimizer with specific learning rates and conducted 5-fold cross-validation. The experiments were implemented in PyTorch using a single GPU.</p>
<h2 id="quantitative-evaluation"><a class="header" href="#quantitative-evaluation">Quantitative Evaluation</a></h2>
<p>The authors evaluated the first re-labeling strategy using the machine annotator. They re-labeled the LIDC nodules using a trained model and tested the performance on a different dataset. The results showed that although the re-labeling fixed the label bias, the model trained with these new labels did not perform well on the test dataset. This indicates that the best model optimized with fine-tuning did not improve the re-labeling performance.</p>
<h2 id="lidc-re-labeling-strategy"><a class="header" href="#lidc-re-labeling-strategy">LIDC re-labeling Strategy</a></h2>
<p>The authors evaluated the performance of different re-labeling methods based on each mode of the re-labeling strategies. They used under-labeled LIDC data with original average scores to compare the methods. The results are presented in a table, showing the sensitivity, specificity, precision, accuracy, and F1 score for each method.</p>
<p>Overall, the authors propose re-labeling strategies to correct bias in the assessment of nodules and improve the performance of pulmonary cancer prediction models. They conduct experiments and provide quantitative evaluations to support their findings.</p>
<p>Title: Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction</p>
<p>Introduction:
This study addresses the issue of low confidence labels in the LIDC-IDRI database, which is a public database of lung nodules. The authors propose a metric learning-based approach to re-label the LIDC data, aiming to improve performance and increase utilization of the database.</p>
<p>Re-labeling Process:
The authors conducted experiments using two modes: Mode 1 (Substitute) and Mode 2 (Consensus). Mode 2 achieved better overall outcomes but with low specificity. Metric learning, which retrieves similar nodules based on a distance metric, showed better performance compared to general learning. Re-labeling uncertain nodules played a significant role in improving outcomes.</p>
<p>Trade-off between Modes:
There is a trade-off between Mode 1 and Mode 2. Mode 2 tends to retain the bias of the LIDC database, resulting in low specificity and data reduction. However, re-labeling uncertain nodules can help mitigate this bias.</p>
<p>Results:
The authors re-labeled the LIDC database using a Siamese Network trained on another dataset. The re-labeled results aligned well with low malignancy scores. Nodules with an average score of 3 (uncertain data) were mostly re-labeled as benign, leading to improved performance. The re-labeling corrected more than half of the original labels with a score of 4, addressing data bias.</p>
<p>Discussion:
Re-labeling through metric learning differs from general supervised models in two ways: it provides data augmentation and increases label confidence by considering top-ranked similar nodules. This explains why metric learning outperforms general supervised models in re-labeling.</p>
<p>Limitations and Future Work:
The lack of pathological ground truth in the LIDC database makes the re-labeling results uncertain. Clinical information is needed to validate the re-labeling outcomes. The authors emphasize the importance of collecting a large pathological-proven nodule database to address future issues.</p>
<p>Conclusion:
Deep learning models trained on the LIDC database have poor generalization capability due to the absence of clinical information. Re-labeling the LIDC data using a metric learning-based approach improves performance and utilization of the database. Future work involves collecting a large pathological-proven nodule database to enhance accuracy and reliability.</p>
<p>References:
The text includes references to various studies and papers related to lung nodule classification and deep learning techniques.</p>
<p>This text is a list of references for a research paper titled &quot;Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction.&quot; The paper discusses various algorithms and techniques used for the detection and classification of pulmonary nodules in computed tomography (CT) images.</p>
<p>Here are some simplified points about the text:</p>
<ul>
<li>The paper references several studies and papers that have contributed to the field of pulmonary nodule detection and classification.</li>
<li>These studies include the Lung Image Database Consortium (LIDC) data collection process, the LUNA16 challenge, and various deep learning approaches.</li>
<li>The National Lung Screening Trial (NLST) is also mentioned, which is a large-scale study that investigated the effectiveness of low-dose CT screening for lung cancer.</li>
<li>The paper discusses the use of transfer learning, multi-scale convolutional neural networks, and ensemble methods for nodule classification.</li>
<li>Some papers focus on specific aspects of nodule classification, such as malignancy prediction and segmentation.</li>
<li>The use of sure data and unsure data for training models is also mentioned.</li>
</ul>
<p>Overall, the text provides a list of relevant references for the research paper on pulmonary nodule detection and classification.</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<p>Vector Search with OpenAI Embeddings: Lucene Is All You Need</p>
<h3 id="authors"><a class="header" href="#authors">Authors</a></h3>
<p>Jimmy Lin, Ronak Pradeep, Tommaso Teofili, Jasper Xian</p>
<h3 id="affiliations"><a class="header" href="#affiliations">Affiliations</a></h3>
<ol>
<li>David R. Cheriton School of Computer Science, University of Waterloo</li>
<li>Department of Engineering, Roma Tre University</li>
</ol>
<h2 id="abstract"><a class="header" href="#abstract">Abstract</a></h2>
<p>This paper demonstrates vector search using OpenAI embeddings and Lucene on the MS MARCO passage ranking test collection. It challenges the idea that a dedicated vector store is necessary for utilizing deep neural networks in search. The paper shows that Lucene's hierarchical navigable small-world network (HNSW) indexes are sufficient for vector search in a bi-encoder architecture. This suggests that there is no need to introduce a separate vector store into a modern AI stack for search, as existing infrastructure can handle it.</p>
<h2 id="1-introduction-1"><a class="header" href="#1-introduction-1">1 Introduction</a></h2>
<p>Recent advancements in deep neural networks have led to the use of dense vectors (embeddings) for representing content in search. This paper argues against the prevailing belief that a dedicated vector store is required for managing these dense vectors. It points out that the open-source Lucene search library, which is widely used in production infrastructure, already includes capabilities for vector search. The paper also mentions the growing popularity of embedding APIs, which simplify the generation of dense vectors. To support its arguments, the paper demonstrates vector search using OpenAI embeddings on the MS MARCO passage ranking test collection.</p>
<h2 id="2-from-architecture-to-implementation"><a class="header" href="#2-from-architecture-to-implementation">2 From Architecture to Implementation</a></h2>
<p>The bi-encoder architecture encodes queries and passages into dense vectors (embeddings) and uses the dot product of these embeddings to compute relevance scores. This allows search to be treated as a nearest neighbor search problem in vector space. Hierarchical navigable small-world networks (HNSW) are commonly used for indexing and searching dense vectors. The Faiss library provides an implementation of HNSW indexes. While sparse and dense retrieval models require different techniques, hybrid approaches that combine both have shown to be effective. The paper mentions the Pyserini IR toolkit, which integrates Lucene and Faiss for sparse and dense retrieval.</p>
<p>The current belief is that a dedicated vector store is necessary for managing both sparse and dense retrieval models in a modern AI stack.</p>
<h1 id="vector-search-with-openai-embeddings-lucene-is-all-you-need"><a class="header" href="#vector-search-with-openai-embeddings-lucene-is-all-you-need">Vector Search with OpenAI Embeddings: Lucene Is All You Need</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>This text discusses the use of vector search with OpenAI embeddings and argues that Lucene, a widely-used search library, is sufficient for implementing vector search. The text also presents experimental results to support this claim.</p>
<h2 id="paragraph-1"><a class="header" href="#paragraph-1">Paragraph 1</a></h2>
<p>Many startups today rely on vector stores to handle operations like CRUD and nearest neighbor search. Examples of such startups include Pinecone, Weaviate, Chroma, Milvus, and Qdrant. This challenges the idea that vector search requires a separate vector store.</p>
<h2 id="paragraph-2"><a class="header" href="#paragraph-2">Paragraph 2</a></h2>
<p>The document provides a link to a detailed document about the experiments conducted using OpenAI embeddings and Lucene.</p>
<h2 id="paragraph-3"><a class="header" href="#paragraph-3">Paragraph 3</a></h2>
<p>Adding a vector store to an existing enterprise architecture can increase complexity and interaction with other components. While vector stores offer new capabilities, it is important to consider if these capabilities can be achieved through alternative means. The term &quot;brownfield application&quot; refers to the development of new software systems alongside existing legacy software applications/systems.</p>
<h2 id="paragraph-4"><a class="header" href="#paragraph-4">Paragraph 4</a></h2>
<p>The latest release of Lucene (version 9) includes HNSW indexing and search capabilities, making the performance difference between Lucene and dedicated vector stores the primary distinction. This suggests that Lucene alone can fulfill the requirements of vector search.</p>
<h2 id="paragraph-5"><a class="header" href="#paragraph-5">Paragraph 5</a></h2>
<p>The experimental results on the MS MARCO passage ranking test collection demonstrate that using OpenAI's ada2 embeddings with Lucene achieves comparable effectiveness to state-of-the-art methods.</p>
<h2 id="paragraph-6"><a class="header" href="#paragraph-6">Paragraph 6</a></h2>
<p>The experiments were conducted using Anserini, a Lucene-based IR toolkit that aims to bridge the gap between academic research and real-world search applications. Anserini allows researchers to easily translate their work into platforms like Elasticsearch.</p>
<h2 id="paragraph-7"><a class="header" href="#paragraph-7">Paragraph 7</a></h2>
<p>The experiments show that state-of-the-art vector search can be implemented by combining readily available components. The logical scoring model maps to the OpenAI embedding API, while the physical retrieval model is handled by Lucene. This makes vector search accessible to a wider audience.</p>
<h2 id="paragraph-8"><a class="header" href="#paragraph-8">Paragraph 8</a></h2>
<p>The experiments focused on the MS MARCO passage ranking test collection, which consists of approximately 8.8 million passages from the web. No model training was performed as the embeddings were generated using OpenAI's API endpoint.</p>
<h2 id="paragraph-9"><a class="header" href="#paragraph-9">Paragraph 9</a></h2>
<p>The experiments used the OpenAI ada2 model for generating query and passage embeddings. The model has an input limit of 8191 tokens and an output embedding size of 1536 dimensions. The passages in the corpus were truncated to 512 tokens for consistency with previous literature.</p>
<h2 id="paragraph-10"><a class="header" href="#paragraph-10">Paragraph 10</a></h2>
<p>The average token count per passage in the corpus was computed to be 75.2. The embeddings were generated efficiently by querying the API in parallel while respecting the rate limit. Error handling logic was incorporated to handle the high volume of API calls.</p>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>The experiments demonstrate that Lucene is sufficient for implementing vector search with OpenAI embeddings. This approach eliminates the need for a separate vector store and simplifies the implementation process.
In this paragraph, the author discusses their experiments with the Anserini IR toolkit and Lucene for vector search using OpenAI embeddings. They mention that Lucene restricts vectors to 1024 dimensions, which was not enough for OpenAI's 1536-dimensional embeddings. They found a workaround for this limitation. They also provide experimental results in terms of effectiveness metrics. They compare the effectiveness of OpenAI's ada2 embeddings to other models. They mention that the results may vary slightly due to the non-deterministic nature of HNSW indexing. They also provide performance figures for their server setup. The author then discusses some issues and considerations related to their demonstration, including the need for official releases of Lucene with all the necessary features and the performance comparison with other alternatives. They mention that Lucene is slower in terms of indexing speed and query throughput compared to alternatives like Faiss, but they believe that Lucene has room for performance improvements and the performance gap will decrease over time. They also mention fully managed services like Vespa as potential alternatives.
Vector search capabilities have become important in text search platforms like Elasticsearch and relational databases. One tool, pgvector, allows vector similarity search in Postgres, making it a convenient option for enterprises already using Postgres. The debate is ongoing about how to implement and deploy vector search capabilities in production systems. Some argue for a separate vector store, while others propose using the Lucene ecosystem, which is already widely used in search applications. Time will tell which approach is best.</p>
<p>References:</p>
<ul>
<li>Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023. Retrieval-based Language Models and Applications.</li>
<li>Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated Machine Reading Comprehension Dataset.</li>
<li>Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview of the TREC 2020 Deep Learning Track.</li>
<li>Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2019. Overview of the TREC 2019 Deep Learning Track.</li>
<li>Josh Devins, Julie Tibshirani, and Jimmy Lin. 2022. Aligning the Research and Practice of Building Search Applications: Elasticsearch and Pyserini.</li>
<li>Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2022. From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective.</li>
<li>Sebastian Hofstitter, Sheng-Chieh Lin, Jneng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling.</li>
<li>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards Unsupervised Dense Information Retrieval with Contrastive Learning.</li>
<li>Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs.</li>
<li>Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. Evaluating Embedding APIs for Information Retrieval.</li>
<li>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering.</li>
<li>Jimmy Lin. 2021. A Proposed Conceptual Framework for a Representational Approach to Information Retrieval.</li>
<li>Jimmy Lin. 2022. Building a Culture of Reproducibility in Academic Research.</li>
<li>Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jneng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations.</li>
<li>Sheng-Chieh Lin, Minghan Li, and Jimmy Lin. 2023. Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval.</li>
<li>Sheng-Chieh Lin and Jimmy Lin. 2023. A Dense Representation Framework for Lexical and Semantic Matching.</li>
<li>Sheng-Chieh Lin, Jneng-Hong Yang, and Jimmy Lin. 2021b. In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval.</li>
<li>Xueguang Ma, Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2022a. Document Expansions and Learned Sparse Lexical Representations for MS MARCO V1 and V2.</li>
<li>Xueguang Ma, Kai Sun, Ronak Pradeep, Minghan Li, and Jimmy Lin. 2022b. Another Look at DPR: Reproduction of Training and Replication of Retrieval.</li>
<li>Xueguang Ma, Tommaso Teofili, and Jimmy Lin. 2023. Anserini Gets Dense Retrieval: Integration of Lucene's HNSW Indexes.</li>
<li>Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.</li>
<li>Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Ro
This text discusses two research papers related to vector search using OpenAI embeddings. The first paper, titled &quot;Vector Search with OpenAI Embeddings: Lucene Is All You Need,&quot; was written by Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk in 2021. It was presented at the 9th International Conference on Learning Representations (ICLR 2021).</li>
</ul>
<p>The second paper, titled &quot;Anserini: Reproducible Ranking Baselines Using Lucene,&quot; was written by Peilin Yang, Hui Fang, and Jimmy Lin in 2018. It was published in the Journal of Data and Information Quality, Volume 10, Issue 4, and spans Article 16.</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="classification-of-radiological-zones"><a class="header" href="#classification-of-radiological-zones">Classification of Radiological Zones</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>Radiological zoning is a way to visualize the danger of exposure to ionizing radiation that workers may face. It is based on recommendations, limits, and criteria set by the International Commission on Radiological Protection (ICRP).</p>
<h2 id="icrp-recommendations"><a class="header" href="#icrp-recommendations">ICRP Recommendations</a></h2>
<p>The ICRP emphasizes the implementation of its recommendations, which include:</p>
<ul>
<li>Justifying all exposures to radiation</li>
<li>Avoiding unnecessary exposures</li>
<li>Keeping doses as low as reasonably achievable (ALARA)</li>
<li>Respecting annual equivalent dose limits</li>
<li>Applying optimization principles in all work areas</li>
</ul>
<h2 id="dose-limits-set-by-icrp"><a class="header" href="#dose-limits-set-by-icrp">Dose Limits set by ICRP</a></h2>
<p>For exposed workers:</p>
<ul>
<li>Average of 20 mSv over 5 consecutive years</li>
<li>Maximum of 50 mSv in any given year</li>
<li>Maximum of 20 mSv for the lens of the eye</li>
<li>Maximum of 500 mSv for the skin and extremities
For the general public:</li>
<li>Maximum of 1 mSv in a year for the whole body</li>
</ul>
<h2 id="objectives-of-radiological-zoning"><a class="header" href="#objectives-of-radiological-zoning">Objectives of Radiological Zoning</a></h2>
<p>During normal operation, the objectives of radiological zoning are to:</p>
<ul>
<li>Classify zones</li>
<li>Classify individuals into categories</li>
<li>Establish local rules and monitoring</li>
<li>Control real-time exposure</li>
</ul>
<h2 id="classification-of-zones"><a class="header" href="#classification-of-zones">Classification of Zones</a></h2>
<ul>
<li>Monitored Zone: A zone where professional exposure conditions need to be monitored, even if no specific protection measures are normally required.</li>
<li>Controlled Zone: A zone where protection measures or safety provisions are or may be required to control normal exposures or prevent the spread of radioactive contamination.</li>
</ul>
<h2 id="classification-of-individuals-into-categories"><a class="header" href="#classification-of-individuals-into-categories">Classification of Individuals into Categories</a></h2>
<ul>
<li>Category A: Workers who regularly work in controlled zones and meet certain conditions such as medical fitness, basic radiation protection training, medical monitoring, individual dosimetry, and potential exposure exceeding 3/10 of the annual limits.</li>
<li>Category B: Workers who regularly work in monitored zones and require specific radiation protection training for their assigned tasks, with a dose not exceeding 3/10 of the annual limits.</li>
<li>Public: Individuals who are neither in Category A nor Category B and are not allowed access to controlled zones.</li>
</ul>
<h2 id="criteria-for-classification-of-controlled-zones"><a class="header" href="#criteria-for-classification-of-controlled-zones">Criteria for Classification of Controlled Zones</a></h2>
<ul>
<li>Dose Rates:
<ul>
<li>Yellow: 10 uSv/h &lt; D &lt; 0.8 mSv/h (1 to 80 times the Derived Air Concentration Limit)</li>
<li>Orange: 0.8 mSv/h &lt; D &lt; 40 mSv/h (Derived Air Concentration Limit)</li>
</ul>
</li>
</ul>
<h2 id="limitation-of-zones"><a class="header" href="#limitation-of-zones">Limitation of Zones</a></h2>
<p>Signage and markings are used in controlled and monitored zones:</p>
<ul>
<li>Green Zone: Permanent access for Category A workers, with the requirement of wearing passive and active dosimeters.</li>
<li>Yellow Zone: Medium radiation risk, with specific radiation protection measures such as contamination protection and time control.</li>
<li>Orange Zone: High radiation risk, with restricted access and mandatory time control.</li>
<li>Red Zone: Normally restricted access.</li>
</ul>
<h2 id="example-of-radiology-installation"><a class="header" href="#example-of-radiology-installation">Example of Radiology Installation</a></h2>
<p>In a radiology room, permanent boundaries are established using walls if possible. The floor may be marked, and signs or movable screens may be used. The zone may change from controlled to monitored when X-ray emission is intermittent, with optical and audible signals to prevent accidental access. The zone may become non-regulated when X-ray emission is not possible.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="./theme/tabbed-code-blocks.js"></script>
        <script src="./theme/highlight.css"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
