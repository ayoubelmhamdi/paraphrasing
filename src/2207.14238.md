# Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction

**Authors:** Hanxiao Zhang, Xiao Gu, Minghui Zhang, Weihao Yu, Liang Chen, Zhexin Wang, Feng Yao, Yun Gu, and Guang-Zhong Yang

**Affiliations:**
1. Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, China
2. Imperial College London, London, UK
3. Department of Thoracic Surgery, Shanghai Chest Hospital, Shanghai Jiao Tong University, Shanghai, China
4. Shanghai Center for Brain Science and Brain-Inspired Technology, Shanghai, China

**Abstract:**
The LIDC-IDRI database is commonly used for lung cancer prediction. However, the subjective assessment of nodules by radiologists in this database can lead to label assignment errors and bias during training. To address this, we propose re-labeling the LIDC data using a small dataset of nodules diagnosed by pathological examination. We demonstrate that re-labeling the LIDC nodules using metric learning can improve model performance. We also suggest that building a larger database of nodules with pathological-proven labels is a long-term solution for robust lung cancer prediction.

**Keywords:**
Pulmonary nodule, Cancer prediction, Metric learning, Re-labeling.

## 1 Introduction
The LIDC-IDRI database is widely used for lung nodule detection and cancer prediction. However, there are potential issues with the subjective assessment of nodules by radiologists in this database. The assigned malignancy scores may not accurately reflect the true nature of the nodules, leading to label assignment errors and uncertainty. To address this, we introduce the SCH-LND dataset, which contains nodules with pathological-proven labels. We evaluate the performance of the LIDC-driven model using the SCH-LND dataset and propose re-labeling strategies based on interactions with the SCH-LND dataset. We show that re-labeling the LIDC nodules using metric learning improves model performance, especially when new labels are added to the uncertain subset. We also highlight the need for a larger nodule database with pathological-proven labels.

## Materials
### LIDC-IDRI Database
We selected nodules identified by at least three radiologists and excluded CT scans with a slice thickness larger than 3 mm. We focused on solid nodules in both the SCH-LND and LIDC databases.
### Extra Dataset: SCH-LND
The SCH-LND dataset consists of 180 solid nodules, with 90 benign and 90 malignant nodules. Each nodule in this dataset has been confirmed and diagnosed through immediate pathological examination via biopsy.

## Study Design
We conducted training and testing in different scenarios and cases to evaluate the performance of the LIDC-driven model in predicting nodule malignancy. We also explored the use of transfer learning to fine-tune the model based on pre-trained LIDC models. We proposed two re-labeling strategies for the LIDC nodules based on interactions with the SCH-LND dataset. The models trained with re-labeled LIDC data created by the metric learning model outperformed the original model, especially when new labels were added to the uncertain subset. Statistical analysis revealed a class imbalance problem in the re-labeled LIDC data, highlighting the need for a larger nodule database with pathological-proven labels.

## Performance Comparisons


# Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction

## Performance Comparisons

The authors compare the performance of different scenarios and cases in predicting whether nodules are malignant. They use various evaluation metrics such as sensitivity, specificity, precision, accuracy, and F1 score. The results show that there is a bias problem in some scenarios, but others have less bias. They also find that training from scratch on a different dataset does not result in a high-performing model. Therefore, they use transfer learning to fine-tune a pre-trained model.

## Relabeling Strategies

The authors propose two strategies to obtain new ground truth labels for the LIDC database. These strategies aim to correct the bias in the assessment of nodules and utilize uncertain nodules with an average score of 3. The details of these strategies are explained in the following section.

## Methods

The authors present two re-labeling strategies to obtain new ground truth labels for the LIDC database. The first strategy generates the malignancy label using a machine annotator that has been trained on LIDC data and fine-tuned on a different dataset. The second strategy involves ranking the labels of top nodules using a metric-based network that measures the correlation between pairs of nodules. Two modes of re-labeling are proposed in each strategy.

For Mode 1 (Substitute), the re-label outcomes from other label machines are accepted. For Mode 2 (Consensus), the final re-label results are decided by the consensus of label machine outcomes and the original label. This mode leaves behind nodules with the same label and discards controversial ones, which may reduce the amount of data.

The effectiveness of the re-labeling is evaluated using a different dataset to test the model trained with the re-labeled data.

## A Machine Annotator: The State-of-the-Art Nodule Classifier

The machine annotator is a state-of-the-art nodule classifier that has been pre-trained on LIDC data and fine-tuned on a different dataset to predict nodule class. The second strategy involves using a metric learning model to search for the most similar nodules and give new labels. The knowledge from radiologists' assessments is also considered in the re-labeling process.

## Label Induction Using Machine Annotator

The authors use a Siamese Network to train a machine annotator. The network takes pairs of nodules and their labels as inputs and generates feature vectors for each nodule. The goal is to learn representations that can distinguish between nodules of the same class and different classes. During re-labeling, the machine annotator is used to pair each nodule from the training dataset with an under-labeled LIDC nodule and assign a new label based on the similarity scores.

## Experiments and Results

The authors conducted experiments to evaluate the re-labeling strategies. They used different evaluation metrics to measure the performance of each method. The results are presented in tables, showing the sensitivity, specificity, precision, accuracy, and F1 score for each method.

## Implementation

The authors used a 3D ResNet-18 model for their experiments. They trained the models using the Adam optimizer with specific learning rates and conducted 5-fold cross-validation. The experiments were implemented in PyTorch using a single GPU.

## Quantitative Evaluation

The authors evaluated the first re-labeling strategy using the machine annotator. They re-labeled the LIDC nodules using a trained model and tested the performance on a different dataset. The results showed that although the re-labeling fixed the label bias, the model trained with these new labels did not perform well on the test dataset. This indicates that the best model optimized with fine-tuning did not improve the re-labeling performance.

## LIDC re-labeling Strategy

The authors evaluated the performance of different re-labeling methods based on each mode of the re-labeling strategies. They used under-labeled LIDC data with original average scores to compare the methods. The results are presented in a table, showing the sensitivity, specificity, precision, accuracy, and F1 score for each method.

Overall, the authors propose re-labeling strategies to correct bias in the assessment of nodules and improve the performance of pulmonary cancer prediction models. They conduct experiments and provide quantitative evaluations to support their findings.


Title: Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction

Introduction:
This study addresses the issue of low confidence labels in the LIDC-IDRI database, which is a public database of lung nodules. The authors propose a metric learning-based approach to re-label the LIDC data, aiming to improve performance and increase utilization of the database.

Re-labeling Process:
The authors conducted experiments using two modes: Mode 1 (Substitute) and Mode 2 (Consensus). Mode 2 achieved better overall outcomes but with low specificity. Metric learning, which retrieves similar nodules based on a distance metric, showed better performance compared to general learning. Re-labeling uncertain nodules played a significant role in improving outcomes.

Trade-off between Modes:
There is a trade-off between Mode 1 and Mode 2. Mode 2 tends to retain the bias of the LIDC database, resulting in low specificity and data reduction. However, re-labeling uncertain nodules can help mitigate this bias.

Results:
The authors re-labeled the LIDC database using a Siamese Network trained on another dataset. The re-labeled results aligned well with low malignancy scores. Nodules with an average score of 3 (uncertain data) were mostly re-labeled as benign, leading to improved performance. The re-labeling corrected more than half of the original labels with a score of 4, addressing data bias.

Discussion:
Re-labeling through metric learning differs from general supervised models in two ways: it provides data augmentation and increases label confidence by considering top-ranked similar nodules. This explains why metric learning outperforms general supervised models in re-labeling.

Limitations and Future Work:
The lack of pathological ground truth in the LIDC database makes the re-labeling results uncertain. Clinical information is needed to validate the re-labeling outcomes. The authors emphasize the importance of collecting a large pathological-proven nodule database to address future issues.

Conclusion:
Deep learning models trained on the LIDC database have poor generalization capability due to the absence of clinical information. Re-labeling the LIDC data using a metric learning-based approach improves performance and utilization of the database. Future work involves collecting a large pathological-proven nodule database to enhance accuracy and reliability.

References:
The text includes references to various studies and papers related to lung nodule classification and deep learning techniques.


This text is a list of references for a research paper titled "Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction." The paper discusses various algorithms and techniques used for the detection and classification of pulmonary nodules in computed tomography (CT) images.

Here are some simplified points about the text:

- The paper references several studies and papers that have contributed to the field of pulmonary nodule detection and classification.
- These studies include the Lung Image Database Consortium (LIDC) data collection process, the LUNA16 challenge, and various deep learning approaches.
- The National Lung Screening Trial (NLST) is also mentioned, which is a large-scale study that investigated the effectiveness of low-dose CT screening for lung cancer.
- The paper discusses the use of transfer learning, multi-scale convolutional neural networks, and ensemble methods for nodule classification.
- Some papers focus on specific aspects of nodule classification, such as malignancy prediction and segmentation.
- The use of sure data and unsure data for training models is also mentioned.

Overall, the text provides a list of relevant references for the research paper on pulmonary nodule detection and classification.


